{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import geom.hyperboloid as hyperboloid\n",
    "import geom.poincare as poincare\n",
    "from learning.frechet import Frechet\n",
    "from learning.pca import TangentPCA, EucPCA, PGA, HoroPCA, BSA\n",
    "from utils.data import load_graph, load_embeddings\n",
    "from utils.metrics import avg_distortion_measures, compute_metrics, format_metrics, aggregate_metrics\n",
    "from utils.sarkar import sarkar, pick_root\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s %(levelname)-8s %(message)s\",\n",
    "        level=logging.INFO,\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    "    )\n",
    "\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph_data(dataset=\"smalltree\"):\n",
    "    graph = load_graph(dataset)\n",
    "    n_nodes = graph.number_of_nodes()\n",
    "    nodelist = np.arange(n_nodes)\n",
    "    graph_dist = torch.from_numpy(nx.floyd_warshall_numpy(graph, nodelist=nodelist))\n",
    "\n",
    "    return graph, graph_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyperbolic_embeddings(graph,dataset=\"smalltree\",dim=10 ,use_sarkar=False, sarkar_scale=3.5):\n",
    "    if use_sarkar:\n",
    "    # embed with Sarkar\n",
    "        logging.info(\"Using sarkar embeddings\")\n",
    "        root = pick_root(graph)\n",
    "        z = sarkar(graph, tau=sarkar_scale, root=root, dim=dim)\n",
    "        z = torch.from_numpy(z)\n",
    "        z_dist = poincare.pairwise_distance(z) / sarkar_scale\n",
    "        return z, z_dist\n",
    "    else:\n",
    "        # load pre-trained embeddings\n",
    "        logging.info(\"Using optimization-based embeddings\")\n",
    "        assert dim in [2, 10, 50], \"pretrained embeddings are only for 2, 10 and 50 dimensions\"\n",
    "        z = load_embeddings(dataset, dim=dim)\n",
    "        z = torch.from_numpy(z)\n",
    "        z_dist = poincare.pairwise_distance(z)\n",
    "        return z, z_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(dataset=\"smalltree\", \n",
    "        model_type=\"horopca\",\n",
    "        metrics_final=[\"distortion\", \"frechet_var\"],\n",
    "        dim=10,\n",
    "        n_components=2, \n",
    "        n_runs=5,\n",
    "        use_sarkar=False,\n",
    "        sarkar_scale=3.5, lr=5e-2):\n",
    "\n",
    "    config = {\n",
    "        \"metrics_final\": metrics_final,\n",
    "        \"dim\": dim,\n",
    "        \"n_components\": n_components,\n",
    "        \"n_runs\": n_runs,\n",
    "        \"use_sarkar\": use_sarkar,\n",
    "        \"sarkar_scale\": sarkar_scale,\n",
    "        \"lr\": lr\n",
    "        }\n",
    "\n",
    "    metrics = []\n",
    "    embeddings = {}\n",
    "    logging.info(f\"Running experiments for {dataset} dataset.\")\n",
    "\n",
    "    graph, graph_dist = load_graph_data(dataset)\n",
    "    z, z_dist = get_hyperbolic_embeddings(graph, dataset, dim, use_sarkar, sarkar_scale)\n",
    "    \n",
    "    # compute embeddings' distortion\n",
    "    distortion = avg_distortion_measures(graph_dist, z_dist)[0]\n",
    "    logging.info(\"Embedding distortion in {} dimensions: {:.4f}\".format(dim, distortion))\n",
    "\n",
    "    # Compute the mean and center the data\n",
    "    logging.info(\"Computing the Frechet mean to center the embeddings\")\n",
    "    frechet = Frechet(lr=1e-2, eps=1e-5, max_steps=5000)\n",
    "    mu_ref, has_converged = frechet.mean(z, return_converged=True)\n",
    "    logging.info(f\"Mean computation has converged: {has_converged}\")\n",
    "    x = poincare.reflect_at_zero(z, mu_ref)\n",
    "\n",
    "    pca_models = {\n",
    "        'pca': {'class': EucPCA, 'optim': False, 'iterative': False, \"n_runs\": 1},\n",
    "        'tpca': {'class': TangentPCA, 'optim': False, 'iterative': False, \"n_runs\": 1},\n",
    "        'pga': {'class': PGA, 'optim': True, 'iterative': True, \"n_runs\": n_runs},\n",
    "        'bsa': {'class': BSA, 'optim': True, 'iterative': False, \"n_runs\": n_runs},\n",
    "        'horopca': {'class': HoroPCA, 'optim': True, 'iterative': False, \"n_runs\": n_runs},\n",
    "    }\n",
    "\n",
    "    if model_type in pca_models.keys():\n",
    "        model_params = pca_models[model_type]\n",
    "        for _ in range(model_params[\"n_runs\"]):\n",
    "            model = model_params['class'](dim=dim, n_components=n_components, lr=lr, max_steps=500)\n",
    "            # if torch.cuda.is_available():\n",
    "            #     model.cuda()\n",
    "            model.fit(x, iterative=model_params['iterative'], optim=model_params['optim'])\n",
    "            metrics.append(model.compute_metrics(x))\n",
    "            embeddings = model.map_to_ball(x).detach().cpu().numpy()\n",
    "        metrics = aggregate_metrics(metrics)\n",
    "    else:\n",
    "        # run hMDS baseline\n",
    "        logging.info(f\"Running hMDS\")\n",
    "        x_hyperboloid = hyperboloid.from_poincare(x)\n",
    "        distances = hyperboloid.distance(x.unsqueeze(-2), x.unsqueeze(-3))\n",
    "        D_p = poincare.pairwise_distance(x)\n",
    "        x_h = hyperboloid.mds(D_p, d=n_components)\n",
    "        x_proj = hyperboloid.to_poincare(x_h)\n",
    "        embeddings[\"hMDS\"] = x_proj.numpy()\n",
    "        metrics = compute_metrics(x, x_proj)\n",
    "\n",
    "    logging.info(f\"Experiments for {dataset} dataset completed.\")\n",
    "    logging.info(\"Computing evaluation metrics\")\n",
    "    results = format_metrics(metrics, metrics_final)\n",
    "    for line in results:\n",
    "        logging.info(line)\n",
    "    return metrics, config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-11 10:49:22 INFO     Running experiments for smalltree dataset.\n",
      "2024-01-11 10:49:22 INFO     Using optimization-based embeddings\n",
      "2024-01-11 10:49:22 INFO     Embedding distortion in 10 dimensions: 0.0222\n",
      "2024-01-11 10:49:22 INFO     Computing the Frechet mean to center the embeddings\n",
      "2024-01-11 10:49:22 INFO     Mean computation has converged: True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: smalltree\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m list_dataset:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset:\u001b[39m\u001b[38;5;124m\"\u001b[39m, dataset)\n\u001b[0;32m----> 8\u001b[0m     metrics, config \u001b[38;5;241m=\u001b[39m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhoropca\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m\"\u001b[39m : dataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetrics\u001b[39m\u001b[38;5;124m\"\u001b[39m: metrics, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m: config})\n",
      "Cell \u001b[0;32mIn[27], line 52\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(dataset, model_type, metrics_final, dim, n_components, n_runs, use_sarkar, sarkar_scale, lr)\u001b[0m\n\u001b[1;32m     49\u001b[0m model \u001b[38;5;241m=\u001b[39m model_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m](dim\u001b[38;5;241m=\u001b[39mdim, n_components\u001b[38;5;241m=\u001b[39mn_components, lr\u001b[38;5;241m=\u001b[39mlr, max_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# if torch.cuda.is_available():\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m#     model.cuda()\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterative\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43miterative\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moptim\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m metrics\u001b[38;5;241m.\u001b[39mappend(model\u001b[38;5;241m.\u001b[39mcompute_metrics(x))\n\u001b[1;32m     54\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmap_to_ball(x)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/Works/github/MML-HoroPCA/learning/pca.py:167\u001b[0m, in \u001b[0;36mPCA.fit\u001b[0;34m(self, x, iterative, optim)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Finds principal components using optimization or spectral decomposition approaches.\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \n\u001b[1;32m    161\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03m    optim: boolean (true to find components via optimization, defaults to SVD otherwise)\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optim:\n\u001b[0;32m--> 167\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_optim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterative\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_spectral(x)\n",
      "File \u001b[0;32m~/Works/github/MML-HoroPCA/learning/pca.py:128\u001b[0m, in \u001b[0;36mPCA.fit_optim\u001b[0;34m(self, x, iterative)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_steps):\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;66;03m# Forward pass: compute _projected variance\u001b[39;00m\n\u001b[1;32m    127\u001b[0m     Q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_components()\n\u001b[0;32m--> 128\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mQ\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m     loss_vals\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m    130\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/Works/github/MML-HoroPCA/learning/pca.py:302\u001b[0m, in \u001b[0;36mHoroPCA.compute_loss\u001b[0;34m(self, x, Q)\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28msum\u001b[39m(auc)\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 302\u001b[0m     proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_project\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mQ\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m     var \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_variance(proj)\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39mvar\n",
      "File \u001b[0;32m~/Works/github/MML-HoroPCA/learning/pca.py:268\u001b[0m, in \u001b[0;36mHoroPCA._project\u001b[0;34m(self, x, Q)\u001b[0m\n\u001b[1;32m    266\u001b[0m     hyperboloid_ideals \u001b[38;5;241m=\u001b[39m hyperboloid\u001b[38;5;241m.\u001b[39mfrom_poincare(Q, ideal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    267\u001b[0m     hyperboloid_x \u001b[38;5;241m=\u001b[39m hyperboloid\u001b[38;5;241m.\u001b[39mfrom_poincare(x)\n\u001b[0;32m--> 268\u001b[0m     hyperboloid_proj \u001b[38;5;241m=\u001b[39m \u001b[43mhyperboloid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhoro_projection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhyperboloid_ideals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperboloid_x\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    269\u001b[0m     proj \u001b[38;5;241m=\u001b[39m hyperboloid\u001b[38;5;241m.\u001b[39mto_poincare(hyperboloid_proj)\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Works/github/MML-HoroPCA/geom/hyperboloid.py:179\u001b[0m, in \u001b[0;36mhoro_projection\u001b[0;34m(ideals, x)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# Find a tangent vector of the hyperboloid at spine_ortho_proj that is tangent to the target submanifold\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# and orthogonal to the spine.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m# This is done in a Gram-Schmidt way: Take the Euclidean vector pointing from spine_ortho_proj to poincare_origin,\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m# then subtract a projection part so that it is orthogonal to the spine and tangent to the hyperboloid\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# Everything below has shape (batch_size, Minkowski_dim)\u001b[39;00m\n\u001b[1;32m    178\u001b[0m chords \u001b[38;5;241m=\u001b[39m poincare_origin \u001b[38;5;241m-\u001b[39m spine_ortho_proj\n\u001b[0;32m--> 179\u001b[0m tangents \u001b[38;5;241m=\u001b[39m chords \u001b[38;5;241m-\u001b[39m \u001b[43mminkowski\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morthogonal_projection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mideals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchords\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m unit_tangents \u001b[38;5;241m=\u001b[39m tangents \u001b[38;5;241m/\u001b[39m torch\u001b[38;5;241m.\u001b[39msqrt(minkowski\u001b[38;5;241m.\u001b[39msquared_norm(tangents))\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    182\u001b[0m proj_1 \u001b[38;5;241m=\u001b[39m exp_unit_tangents(spine_ortho_proj, unit_tangents, spine_dist)\n",
      "File \u001b[0;32m~/Works/github/MML-HoroPCA/geom/minkowski.py:60\u001b[0m, in \u001b[0;36morthogonal_projection\u001b[0;34m(basis, x, bilinear_form)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m@\u001b[39m bilinear_form \u001b[38;5;241m@\u001b[39m y\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21morthogonal_projection\u001b[39m(basis, x, bilinear_form\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     61\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the orthogonal projection of x onto the vector subspace spanned by basis.\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;03m    Here orthogonality is defined using the given bilinear_form\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m    If bilinear_form is not provided, use the default Minkowski form,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03m             (In that case, the orthogonal projection is not unique)\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;66;03m# print(basis.size(), x.size()    )\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "list_dataset = [\"smalltree\", \"phylo-tree\", \"ca-CSphd\", \"bio-diseasome\"]\n",
    "list_model = [\"pca\", \"horopca\", \"pga\", \"bsa\", \"tpca\", \"hmds\"]\n",
    "\n",
    "results = []\n",
    "\n",
    "for dataset in list_dataset:\n",
    "    print(\"Dataset:\", dataset)\n",
    "    metrics, config = run(dataset=dataset, model_type=\"horopca\")\n",
    "    results.append({\"dataset\" : dataset, \"metrics\": metrics, \"config\": config})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
